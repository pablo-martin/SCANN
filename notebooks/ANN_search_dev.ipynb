{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Index kNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data we have so far\n",
    "- we can only index with integers, it would be great to add the path but we can't\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820000, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = \"../src/embeddings/\"\n",
    "\n",
    "batches = list()\n",
    "for _file in os.listdir(base_dir):\n",
    "    if os.path.basename(_file).split(\".\")[-1] == \"parquet\":\n",
    "        batches.append(pd.read_parquet(os.path.join(base_dir, _file)))\n",
    "        \n",
    "df = (\n",
    "    pd.concat(batches)\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\":\"contract_id\"})\n",
    ")\n",
    "print(df.shape)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annoy Library\n",
    "there are 2 tradeoffs for building the index (from the docs)\n",
    "\n",
    "- n_trees is provided during build time and affects the build time and the index size. A larger value will give more accurate results, but larger indexes.\n",
    "- search_k is provided in runtime and affects the search performance. A larger value will give more accurate results, but will take longer time to return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "t = AnnoyIndex(150, 'dot')\n",
    "t.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add items to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.45 s, sys: 23.2 ms, total: 4.48 s\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ix, row in df.iterrows():\n",
    "    t.add_item(ix, row.loc[\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated time for loading all contracts is: 4.032 minutes\n"
     ]
    }
   ],
   "source": [
    "n_batches = int(1083325 / 20000)\n",
    "print(f\"estimated time for loading all contracts is: {n_batches*4.48/60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the index\n",
    "- this is one of the parameters to tune, let's do low multithreading so i don't disturbe the embedding computation\n",
    "- the variables to watch is memory and build time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.55 s, sys: 147 ms, total: 6.69 s\n",
      "Wall time: 2.74 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "t.build(10, n_jobs=3)\n",
    "t.save('test.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56M\t./test.ann\n"
     ]
    }
   ],
   "source": [
    "! du -ah | grep test.ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ok that was very fast and not that much memory for the \"model\"\n",
    "- let's check out how inference works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/7c/7CfFEEbF5d4Bcc701003ebd623a4378b2E023Da4_UniswapTest.sol\n",
      "CPU times: user 1.11 ms, sys: 529 Âµs, total: 1.64 ms\n",
      "Wall time: 1.13 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{30780: '/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/fe/fE520341C202D9170EDe56F4DB763b27F4dc03E4_COIN.sol',\n",
       " 46450: '/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/57/5705b1C4a0Abf33281bEbb090f617e772516C21a_DT.sol',\n",
       " 66772: '/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/b2/b2B811F2B0ca741b2C13Be1d57278AD321c4720c_NfToken.sol',\n",
       " 47767: '/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/6f/6fbbe7c2aad159ad9bb38e4ec2a4e7bbcfbac36e_SmartInvestorGuys.sol',\n",
       " 46359: '/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/57/57d1bc7480a656D030B482456CBAb65d6Cd71B3F_RakuCoin.sol'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "source_contract, embedding = df.loc[12,\"contract_path\"], df.loc[12,\"embedding\"]\n",
    "res = t.get_nns_by_vector(embedding, 5, search_k=20, include_distances=False)\n",
    "print(source_contract)\n",
    "df.loc[res, \"contract_path\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- again very fast, i have no idea if the recommendations are good though...was manually looking through them but it's hard to tell for me\n",
    "- let's package it into a function and try different parameters for build sizes and times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def build_index(df, n_trees=20, n_jobs=3, output_path=None):\n",
    "    st = time.time()\n",
    "    t = AnnoyIndex(150, 'dot')\n",
    "    t.set_seed(42)\n",
    "    for ix, row in df.iterrows():\n",
    "        t.add_item(ix, row.loc[\"embedding\"])\n",
    "    t.build(n_trees, n_jobs=n_jobs)\n",
    "    \n",
    "    if output_path is not None:\n",
    "        t.save(output_path)\n",
    "    elapsed = time.time() - st\n",
    "    return t, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build time for 10 trees was 7.638796091079712 seconds\n",
      "build time for 20 trees was 9.878449201583862 seconds\n",
      "build time for 50 trees was 16.51998209953308 seconds\n",
      "build time for 100 trees was 29.181003093719482 seconds\n",
      "build time for 200 trees was 53.388516902923584 seconds\n"
     ]
    }
   ],
   "source": [
    "indexes = dict()\n",
    "tree_dimensions = (10, 20, 50, 100, 200)\n",
    "for n_trees in tree_dimensions:\n",
    "    t, elapsed = build_index(df, n_trees=n_trees, n_jobs=3, output_path=\"test_{}.ann\".format(n_trees))\n",
    "    indexes[n_trees] = t\n",
    "    print(f\"build time for {n_trees} trees was {elapsed} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56M\t./test_10.ann\n",
      " 66M\t./test_20.ann\n",
      " 95M\t./test_50.ann\n",
      "142M\t./test_100.ann\n",
      "237M\t./test_200.ann\n"
     ]
    }
   ],
   "source": [
    "! du -ah | grep test_ | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_trees: 10, search_k:10 -> inference: 0.020502805709838867 seconds\n",
      "n_trees: 20, search_k:10 -> inference: 0.01873779296875 seconds\n",
      "n_trees: 50, search_k:10 -> inference: 0.020534753799438477 seconds\n",
      "n_trees: 100, search_k:10 -> inference: 0.027746915817260742 seconds\n",
      "n_trees: 200, search_k:10 -> inference: 0.026354074478149414 seconds\n",
      "n_trees: 10, search_k:20 -> inference: 0.00036716461181640625 seconds\n",
      "n_trees: 20, search_k:20 -> inference: 0.0003590583801269531 seconds\n",
      "n_trees: 50, search_k:20 -> inference: 0.0003719329833984375 seconds\n",
      "n_trees: 100, search_k:20 -> inference: 0.0003972053527832031 seconds\n",
      "n_trees: 200, search_k:20 -> inference: 0.0004780292510986328 seconds\n",
      "n_trees: 10, search_k:50 -> inference: 0.0003330707550048828 seconds\n",
      "n_trees: 20, search_k:50 -> inference: 0.0003440380096435547 seconds\n",
      "n_trees: 50, search_k:50 -> inference: 0.0003719329833984375 seconds\n",
      "n_trees: 100, search_k:50 -> inference: 0.00039076805114746094 seconds\n",
      "n_trees: 200, search_k:50 -> inference: 0.0004711151123046875 seconds\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def inference(t, embedding, search_k=20):\n",
    "    st = time.time()\n",
    "    res = t.get_nns_by_vector(embedding, 5, search_k=20, include_distances=False)\n",
    "    similar_contracts = df.loc[res, \"contract_path\"].to_dict()\n",
    "    elapsed = time.time() - st\n",
    "    return similar_contracts, elapsed\n",
    "    \n",
    "embedding = df.loc[23,\"embedding\"]\n",
    "search_ks = (10, 20, 50)\n",
    "for search_k, (n_trees, _t) in product(search_ks, indexes.items()):\n",
    "    similar_contracts, elapsed = inference(_t, embedding, search_k=search_k)\n",
    "    print(f\"n_trees: {n_trees}, search_k:{search_k} -> inference: {elapsed} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- build time seems to scale relatively linearly\n",
    "- memory footprint also scales pretty linearly\n",
    "- inference time is also linear \n",
    "- all of them seem very low so we can try with a full dataset. the only concerning number is the memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm *.ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# consolidate\n",
    "- let's gather all the necessary functions into a file\n",
    "- figure out what's the maximum memory footprint we want to have\n",
    "- the not ideal part is we also need to have the dataframe loaded to get the contract_path to be able to return that. we can drop the embeddings at that point to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "base_dir = \"embeddings/\"\n",
    "\n",
    "def get_batch_names(base_dir : str) -> str:\n",
    "    batches = list()\n",
    "    for _file in os.listdir(base_dir):\n",
    "        if os.path.basename(_file).split(\".\")[-1] == \"parquet\":\n",
    "            yield os.path.join(base_dir, _file)\n",
    "            \n",
    "def generate_dataset(base_dir : str) -> pd.DataFrame:  \n",
    "    return (\n",
    "        pd.concat([pd.read_parquet(batch_file) for batch_file in get_batch_names(base_dir)])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def build_index(df, n_trees=20, n_jobs=3, output_path=None):\n",
    "    st = time.time()\n",
    "    t = AnnoyIndex(150, 'dot')\n",
    "    t.set_seed(42)\n",
    "    for ix, row in df.iterrows():\n",
    "        t.add_item(ix, row.loc[\"embedding\"])\n",
    "    t.build(n_trees, n_jobs=n_jobs)\n",
    "    \n",
    "    if output_path is not None:\n",
    "        t.save(output_path)\n",
    "    elapsed = time.time() - st\n",
    "    return t, elapsed\n",
    "\n",
    "\n",
    "def inference(t, embedding, search_k=20):\n",
    "    st = time.time()\n",
    "    res = t.get_nns_by_vector(embedding, 5, search_k=20, include_distances=False)\n",
    "    similar_contracts = df.loc[res, \"contract_path\"].to_dict()\n",
    "    elapsed = time.time() - st\n",
    "    return similar_contracts, elapsed\n",
    "\n",
    "\n",
    "class IndexBuilder(object):\n",
    "    \n",
    "    def __init__(self, pretrained=None, dataset_dir=None):\n",
    "        # embedding size is fixed\n",
    "        self._index = AnnoyIndex(150, 'dot')\n",
    "        \n",
    "        if pretrained is not None:\n",
    "            self.load_pretrained(pretrained)\n",
    "            \n",
    "        if dataset_dir is not None:\n",
    "            self._data = self.generate_dataset(dataset_dir)\n",
    "    \n",
    "    def load_pretrained(self, pretrained):\n",
    "        self._index.load(pretrained)\n",
    "\n",
    "    def get_batch_names(self, base_dir : str) -> str:\n",
    "        batches = list()\n",
    "        for _file in os.listdir(base_dir):\n",
    "            if os.path.basename(_file).split(\".\")[-1] == \"parquet\":\n",
    "                yield os.path.join(base_dir, _file)\n",
    "\n",
    "    def generate_dataset(self, dataset_dir : str) -> pd.DataFrame:  \n",
    "        return (\n",
    "            pd.concat([pd.read_parquet(batch_file) for batch_file in get_batch_names(dataset_dir)])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    def build_index(self, n_trees=20, n_jobs=3, output_path=None):\n",
    "        assert hasattr(IB, \"_data\"), \"must load data before building index through dataset_dir\"\n",
    "        st = time.time()\n",
    "        self._index.set_seed(42)\n",
    "        for ix, row in self._data.iterrows():\n",
    "            self._index.add_item(ix, row.loc[\"embedding\"])\n",
    "        self._index.build(n_trees, n_jobs=n_jobs)\n",
    "\n",
    "        if output_path is not None:\n",
    "            self._index.save(output_path)\n",
    "        elapsed = time.time() - st\n",
    "        print(f\"time elapsed to build index: {timedelta(seconds=elapsed)}\")\n",
    "        \n",
    "        # we can throw away the embeddings to save memory\n",
    "        self._data = self._data[\"contract_path\"].to_dict()\n",
    "        \n",
    "    def preprocess_contract(self, contract: Union[str, np.array]) -> np.array:\n",
    "        if isinstance(contract, str):\n",
    "            pass\n",
    "        else:\n",
    "            return contract\n",
    "        \n",
    "    def __call__(self, contract, k=5, search_k=20, include_distances=False):\n",
    "        embedding = self.preprocess_contract(contract)\n",
    "        res = self._index.get_nns_by_vector(embedding, k, search_k=20, include_distances=include_distances)\n",
    "        similar_contracts = [self._data[_res] for _res in res]\n",
    "        return similar_contracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed to build index: 0:00:09.295983\n",
      "CPU times: user 18 s, sys: 321 ms, total: 18.3 s\n",
      "Wall time: 9.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "IB = IndexBuilder(dataset_dir=\"embeddings/\")\n",
    "IB.build_index(output_path=\"test20.ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/ca/CAac6F5574d1C959D6B4Ff5c89344bee888fd89E_Trade.sol',\n",
       " '/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/d1/d16Ba886292f20c29d5c31F9756cf40827da40a0_CopygameAxelarBridge.sol',\n",
       " '/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/a4/A4501782232DC056EC4B358cAACbcDEF274f52Ab_NFT.sol',\n",
       " '/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/b5/b52aB99F349F400941311531D9508AE63A4cf218_SkaleDKG.sol',\n",
       " '/Users/pablo/Documents/Coding/company_challenges/OpenZeppelin/smart-contract-sanctuary-ethereum/contracts/ropsten/e3/E3023D7A9984051035191D34c8B149f76F6FE8FE_BondPoolLibV1.sol']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_contracts = IB(embedding)\n",
    "similar_contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeppelin",
   "language": "python",
   "name": "zeppelin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
